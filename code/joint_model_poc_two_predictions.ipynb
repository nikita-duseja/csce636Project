{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2TagDictLanguage = {}\n",
    "word2TagDictNER = {}\n",
    "dictTagsLanguage = {}\n",
    "dictTagsNER = {}\n",
    "data = []\n",
    "\n",
    "#TODO: Read both files here\n",
    "#TODO: there will be 2 separate tag doctionaries\n",
    "#TODO: word vocab dictionary will be common\n",
    "\n",
    "## Language Data \n",
    "with open('final.tsv', 'r') as train_data_language:\n",
    "    tagLine = train_data_language.readline()\n",
    "    while tagLine:\n",
    "        tokens = tagLine.split()\n",
    "        if len(tokens) == 6:\n",
    "            if tokens[0] in dictTagsLanguage:\n",
    "                dictTagsLanguage[tokens[0]][tokens[4]] = tokens[5]\n",
    "            else:\n",
    "                dictTagsLanguage[tokens[0]] = {}\n",
    "                dictTagsLanguage[tokens[0]][tokens[4]] = tokens[5]\n",
    "\n",
    "            word2TagDictLanguage[tokens[4]] = tokens[5]\n",
    "        tagLine = train_data_language.readline()\n",
    "\n",
    "## NER data\n",
    "with open('data.tsv', 'r') as train_data_NER:\n",
    "    tagLine = train_data_NER.readline()\n",
    "    while tagLine:\n",
    "        tokens = tagLine.split()\n",
    "        if len(tokens) == 6:\n",
    "            if tokens[0] in dictTagsNER:\n",
    "                dictTagsNER[tokens[0]][tokens[4]] = tokens[5]\n",
    "            else:\n",
    "                dictTagsNER[tokens[0]] = {}\n",
    "                dictTagsNER[tokens[0]][tokens[4]] = tokens[5]\n",
    "\n",
    "            word2TagDictNER[tokens[4]] = tokens[5]\n",
    "        tagLine = train_data_NER.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Do this for each of the two datsets\n",
    "\n",
    "dataNER = []\n",
    "dataLanguage = []\n",
    "for tweet in dictTagsLanguage:\n",
    "    sentence = dictTagsLanguage[tweet]\n",
    "    listTup = [(k, v) for k, v in sentence.items()]\n",
    "    dataLanguage.append(listTup)\n",
    "    \n",
    "for tweet in dictTagsNER:\n",
    "    sentence = dictTagsNER[tweet]\n",
    "    listTup = [(k, v) for k, v in sentence.items()]\n",
    "    dataNER.append(listTup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: tags will be separate for each type\n",
    "#TODO: words will be combined\n",
    "\n",
    "wordsLanguage = set(word2TagDictLanguage.keys())\n",
    "wordsNER = set(word2TagDictNER.keys())\n",
    "tagsLanguage = list(set(word2TagDictLanguage.values()))\n",
    "tagsNER = list(set(word2TagDictNER.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ambiguous', 'other', 'mixed', 'lang1', 'ne', 'lang2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagsLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-GROUP',\n",
       " 'B-LOC',\n",
       " 'B-TITLE',\n",
       " 'I-PROD',\n",
       " 'I-ORG',\n",
       " 'B-PROD',\n",
       " 'B-EVENT',\n",
       " 'I-OTHER',\n",
       " 'I-LOC',\n",
       " 'I-EVENT',\n",
       " 'I-TIME',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-TIME',\n",
       " 'B-ORG',\n",
       " 'B-GROUP',\n",
       " 'B-OTHER',\n",
       " 'I-TITLE']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagsNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags_language = len(tagsLanguage)\n",
    "num_words_language = len(wordsLanguage)\n",
    "num_words_NER = len(wordsNER)\n",
    "num_tags_NER = len(tagsNER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62515"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(wordsLanguage.union(wordsNER))\n",
    "num_words = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: words are combined, so only 1 dictionary\n",
    "#TODO: tags are 2 sets, so 2 separate dictionaries\n",
    "max_len = 75\n",
    "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "tagLang2idx = {t: i for i, t in enumerate(tagsLanguage)}\n",
    "tagNER2idx = {t: i for i, t in enumerate(tagsNER)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create X_ner, X_lang for the two separate datastes - same word2idx is used\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "XLang = [[word2idx[w[0]] for w in s] for s in dataLanguage]\n",
    "XNER = [[word2idx[w[0]] for w in s] for s in dataNER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "XLang = pad_sequences(maxlen=max_len, sequences=XLang, padding=\"post\", value=num_words-1)\n",
    "XNER = pad_sequences(maxlen=max_len, sequences=XNER, padding=\"post\", value=num_words-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create y_ner, y_lang for the two separate datastes - each has its own tag2idx\n",
    "yLang = [[tagLang2idx[w[1]] for w in s] for s in dataLanguage]\n",
    "yLang = pad_sequences(maxlen=max_len, sequences=yLang, padding=\"post\", value=tagLang2idx[\"other\"])\n",
    "yNER = [[tagNER2idx[w[1]] for w in s] for s in dataNER]\n",
    "yNER = pad_sequences(maxlen=max_len, sequences=yNER, padding=\"post\", value=tagNER2idx[\"O\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "yLang = [to_categorical(i, num_classes=num_tags_language) for i in yLang]\n",
    "yNER = [to_categorical(i, num_classes=num_tags_NER) for i in yNER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_lang_tr, X_lang_te, y_lang_tr, y_lang_te = train_test_split(XLang, yLang, test_size=0.1)\n",
    "X_ner_tr, X_ner_te, y_ner_tr, y_ner_te = train_test_split(XNER, yNER, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = Input(shape=(max_len,))\n",
    "# model = Embedding(input_dim=num_words + 1, output_dim=50,\n",
    "#                   input_length=max_len, mask_zero=True)(input)  # 20-dim embedding\n",
    "# model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "#                            recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "# model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "# crf = CRF(num_tags)  # CRF layer\n",
    "# out = crf(model)  # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## biLSTM for language identification \n",
    "#TODO give a name to the model. will output results to a file based on model_name\n",
    "model_name=\"default\"\n",
    "input = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=num_words, output_dim={{choice([50, 100, 200])}}, input_length=max_len)(input)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units={{choice([50, 100, 200])}}, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out_1 = TimeDistributed(Dense(num_tags_language, activation=\"softmax\"))(model)  # softmax output layer\n",
    "out_2 = TimeDistributed(Dense(num_tags_NER, activation=\"softmax\"))(model)  # softmax output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Model(input, out_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model(input, out_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "callbacksLanguage = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint(filepath='best_cs_joint_lang_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "callbacksNER = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint(filepath='best_cs_joint_ner_model.h5', monitor='val_loss', save_best_only=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 25s 4ms/step - loss: 0.2670 - acc: 0.9220 - val_loss: 0.1020 - val_acc: 0.9679\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 102s 3ms/step - loss: 0.0510 - acc: 0.9915 - val_loss: 0.0239 - val_acc: 0.9954\n",
      "epoch =  1\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0790 - acc: 0.9757 - val_loss: 0.0465 - val_acc: 0.9872\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 0.0189 - acc: 0.9957 - val_loss: 0.0193 - val_acc: 0.9958\n",
      "epoch =  2\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0467 - acc: 0.9868 - val_loss: 0.0381 - val_acc: 0.9899\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 0.0140 - acc: 0.9965 - val_loss: 0.0190 - val_acc: 0.9962\n",
      "epoch =  3\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0360 - acc: 0.9905 - val_loss: 0.0369 - val_acc: 0.9904\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 0.0113 - acc: 0.9972 - val_loss: 0.0184 - val_acc: 0.9965\n",
      "epoch =  4\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0333 - acc: 0.9913 - val_loss: 0.0382 - val_acc: 0.9901\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 0.0091 - acc: 0.9977 - val_loss: 0.0187 - val_acc: 0.9965\n",
      "epoch =  5\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0302 - acc: 0.9921 - val_loss: 0.0387 - val_acc: 0.9902\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.0189 - val_acc: 0.9965\n",
      "epoch =  6\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0269 - acc: 0.9930 - val_loss: 0.0406 - val_acc: 0.9891\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0196 - val_acc: 0.9961\n",
      "epoch =  7\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0240 - acc: 0.9937 - val_loss: 0.0402 - val_acc: 0.9899\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0204 - val_acc: 0.9961\n",
      "epoch =  8\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0215 - acc: 0.9943 - val_loss: 0.0424 - val_acc: 0.9891\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0199 - val_acc: 0.9963\n",
      "epoch =  9\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0191 - acc: 0.9950 - val_loss: 0.0427 - val_acc: 0.9892\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0214 - val_acc: 0.9964\n",
      "epoch =  10\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0436 - val_acc: 0.9889\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0221 - val_acc: 0.9961\n",
      "epoch =  11\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 25s 3ms/step - loss: 0.0156 - acc: 0.9958 - val_loss: 0.0434 - val_acc: 0.9893\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0230 - val_acc: 0.9959\n",
      "epoch =  12\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0141 - acc: 0.9962 - val_loss: 0.0461 - val_acc: 0.9889\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 103s 3ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0231 - val_acc: 0.9961\n",
      "epoch =  13\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.0462 - val_acc: 0.9883\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 9.9892e-04 - acc: 0.9997 - val_loss: 0.0249 - val_acc: 0.9956\n",
      "epoch =  14\n",
      "Train on 7072 samples, validate on 786 samples\n",
      "Epoch 1/1\n",
      "7072/7072 [==============================] - 24s 3ms/step - loss: 0.0117 - acc: 0.9967 - val_loss: 0.0470 - val_acc: 0.9884\n",
      "Train on 29897 samples, validate on 3322 samples\n",
      "Epoch 1/1\n",
      "29897/29897 [==============================] - 101s 3ms/step - loss: 7.8605e-04 - acc: 0.9998 - val_loss: 0.0252 - val_acc: 0.9959\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch = \", epoch)\n",
    "    history_1 = model_1.fit(X_lang_tr, np.array(y_lang_tr), batch_size=32, callbacks=callbacksLanguage, epochs=1, validation_split=0.1, verbose=1)\n",
    "    history_2 = model_2.fit(X_ner_tr, np.array(y_ner_tr), batch_size=32, callbacks=callbacksNER,epochs=1, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model_1.fit(X_lang_tr, np.array(y_lang_tr), batch_size=32, epochs=1, validation_split=0.1, verbose=1)\n",
    "#history_2 = model_2.fit(X_tr, np.array(y_tr), batch_size=32, epochs=1, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Need to load the best model. Currently the last epoch model is running\n",
    "#TODO: For this, save the model while training - at every epoch\n",
    "#TODO: And select model based on validation loss\n",
    "#TODO: We can consider loading different model states for language and different for ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "874/874 [==============================] - 1s 1ms/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         ne       0.69      0.57      0.62       203\n",
      "      lang2       0.68      0.64      0.66       667\n",
      "      other       0.86      0.86      0.86      1888\n",
      "      lang1       0.71      0.74      0.73      1117\n",
      "  ambiguous       0.00      0.00      0.00        27\n",
      "      mixed       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.77      0.77      0.77      3903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate language\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "from keras.models import load_model\n",
    "model_1 = load_model('best_cs_joint_lang_model.h5')\n",
    "test_pred = model_1.predict(X_lang_te, verbose=1)\n",
    "\n",
    "idx2tag = {i: w for w, i in tagLang2idx.items()}\n",
    "\n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i].replace(\"PAD\", \"other\"))\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "    \n",
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = pred2label(y_lang_te)\n",
    "\n",
    "print(classification_report(test_labels, pred_labels))\n",
    "#TODO - print results to a file\n",
    "# name using model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3691/3691 [==============================] - 4s 1ms/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       TIME       0.00      0.00      0.00        52\n",
      "        ORG       0.00      0.00      0.00        54\n",
      "       PROD       0.72      0.18      0.29        99\n",
      "        PER       0.48      0.47      0.48       370\n",
      "      GROUP       0.00      0.00      0.00        77\n",
      "      OTHER       0.00      0.00      0.00        31\n",
      "      TITLE       0.12      0.13      0.13        52\n",
      "        LOC       0.53      0.45      0.49       185\n",
      "      EVENT       0.00      0.00      0.00        15\n",
      "\n",
      "avg / total       0.38      0.30      0.32       935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ner\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "model_2 = load_model('best_cs_joint_ner_model.h5')\n",
    "test_pred = model_2.predict(X_ner_te, verbose=1)\n",
    "\n",
    "idx2tag = {i: w for w, i in tagNER2idx.items()}\n",
    "\n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i].replace(\"PAD\", \"other\"))\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "    \n",
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = pred2label(y_ner_te)\n",
    "\n",
    "print(classification_report(test_labels, pred_labels))\n",
    "#TODO - print results to a file\n",
    "# name using model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
